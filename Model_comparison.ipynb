{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl/ggrMOSXSK1klOuFQVk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuvitKumar003/Pre_trained_Model_comparison_For_text_Similarity/blob/main/Model_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj94asbcLLrG",
        "outputId": "73ecb6a6-69a0-491c-96bd-6d40c12f5631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name xlm-roberta-base. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ranking of Sentence Pairs (Equal Weights, Higher is Better):\n",
            " Rank  S1  S2 TOPSIS Score\n",
            "    1  S1  S1       0.9189\n",
            "    2  S2  S2       0.6549\n",
            "    3  S4  S4       0.5255\n",
            "    4  S7  S7       0.4650\n",
            "    5  S9  S9       0.4646\n",
            "    6  S3  S3       0.4196\n",
            "    7  S5  S5       0.3927\n",
            "    8  S6  S6       0.3496\n",
            "    9  S8  S8       0.2197\n",
            "   10 S10 S10       0.1829\n",
            "\n",
            "Ranking of Sentence Pairs (Custom Weights, Higher is Better):\n",
            " Rank  S1  S2 TOPSIS Score\n",
            "    1  S1  S1       0.9413\n",
            "    2  S2  S2       0.7321\n",
            "    3  S4  S4       0.5010\n",
            "    4  S9  S9       0.4997\n",
            "    5  S3  S3       0.4087\n",
            "    6  S7  S7       0.4052\n",
            "    7  S5  S5       0.3481\n",
            "    8  S6  S6       0.2948\n",
            "    9  S8  S8       0.2271\n",
            "   10 S10 S10       0.1351\n",
            "\n",
            "Best Performing Model (Equal Weights): SBERT\n",
            "Best Performing Model (Custom Weights): USE\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd  # For table formatting\n",
        "\n",
        "# Load Pre-trained Models\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # SBERT\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")  # USE\n",
        "mpnet_model = SentenceTransformer('all-mpnet-base-v2')  # MPNet\n",
        "bert_base_model = SentenceTransformer('bert-base-nli-mean-tokens')  # BERT-NLI\n",
        "roberta_model = SentenceTransformer('all-roberta-large-v1')  # RoBERTa\n",
        "xlm_roberta_model = SentenceTransformer('xlm-roberta-base')  # XLM-RoBERTa\n",
        "bert_large_model = SentenceTransformer('bert-large-nli-mean-tokens')  # BERT-Large\n",
        "\n",
        "# Sample Sentences\n",
        "sentences = [\n",
        "    (\"Thapar Institute offers a course on Predictive Statistics.\", \"Predictive Statistics is part of the curriculum at Thapar Institute.\"),\n",
        "    (\"Machine learning is an important component of Predictive Statistics.\", \"Predictive Statistics includes machine learning techniques.\"),\n",
        "    (\"Students learn regression analysis in Predictive Statistics.\", \"Regression is a fundamental topic in Predictive Statistics at Thapar.\"),\n",
        "    (\"The Predictive Statistics course covers time-series forecasting.\", \"Students explore time-series analysis as part of Predictive Statistics.\"),\n",
        "    (\"Data preprocessing is crucial for accurate predictions.\", \"Cleaning and preprocessing data improves prediction accuracy.\"),\n",
        "    (\"Thapar Institute provides hands-on projects in Predictive Statistics.\", \"Project-based learning enhances understanding of Predictive Statistics.\"),\n",
        "    (\"Predictive models help in decision making.\", \"Statistical models support informed decision-making processes.\"),\n",
        "    (\"Deep learning techniques are sometimes used in Predictive Statistics.\", \"Neural networks enhance the predictive capabilities of statistical models.\"),\n",
        "    (\"Students use Python for data analysis in Predictive Statistics.\", \"Python is widely used for statistical computing and analysis.\"),\n",
        "    (\"The final project in Predictive Statistics involves real-world data.\", \"Students analyze real datasets as part of their final project.\")\n",
        "]\n",
        "\n",
        "# Compute Similarity Scores\n",
        "def compute_similarity(sentence1, sentence2, model):\n",
        "    emb1 = model.encode([sentence1])[0] if isinstance(model, SentenceTransformer) else model([sentence1])[0].numpy()\n",
        "    emb2 = model.encode([sentence2])[0] if isinstance(model, SentenceTransformer) else model([sentence2])[0].numpy()\n",
        "    return 1 - cosine(emb1, emb2)\n",
        "\n",
        "# Build Decision Matrix\n",
        "similarity_matrix = []\n",
        "for s1, s2 in sentences:\n",
        "    similarity_matrix.append([\n",
        "        compute_similarity(s1, s2, sbert_model),\n",
        "        compute_similarity(s1, s2, use_model),\n",
        "        compute_similarity(s1, s2, mpnet_model),\n",
        "        compute_similarity(s1, s2, bert_base_model),\n",
        "        compute_similarity(s1, s2, roberta_model),\n",
        "        compute_similarity(s1, s2, xlm_roberta_model),\n",
        "        compute_similarity(s1, s2, bert_large_model)\n",
        "    ])\n",
        "\n",
        "similarity_matrix = np.array(similarity_matrix)\n",
        "\n",
        "# Normalize Decision Matrix\n",
        "scaler = MinMaxScaler()\n",
        "normalized_matrix = scaler.fit_transform(similarity_matrix)\n",
        "\n",
        "# Function to Apply TOPSIS\n",
        "def apply_topsis(matrix, weights):\n",
        "    weighted_matrix = matrix * weights\n",
        "    ideal_solution = np.max(weighted_matrix, axis=0)\n",
        "    anti_ideal_solution = np.min(weighted_matrix, axis=0)\n",
        "    distance_from_ideal = np.sqrt(np.sum((weighted_matrix - ideal_solution) ** 2, axis=1))\n",
        "    distance_from_anti_ideal = np.sqrt(np.sum((weighted_matrix - anti_ideal_solution) ** 2, axis=1))\n",
        "    closeness = distance_from_anti_ideal / (distance_from_ideal + distance_from_anti_ideal)\n",
        "    ranking = np.argsort(closeness)[::-1]  # Sort in descending order\n",
        "    return closeness, ranking\n",
        "\n",
        "# Define Weights\n",
        "equal_weights = np.array([1/7] * 7)  # Equal weights for all models\n",
        "custom_weights = np.array([0.1, 0.2, 0.2, 0.15, 0.15, 0.1, 0.1])  # Custom importance\n",
        "\n",
        "# Apply TOPSIS for Both Weight Configurations\n",
        "closeness_equal, ranking_equal = apply_topsis(normalized_matrix, equal_weights)\n",
        "closeness_custom, ranking_custom = apply_topsis(normalized_matrix, custom_weights)\n",
        "\n",
        "# Get Model Names\n",
        "model_names = [\"SBERT\", \"USE\", \"MPNet\", \"BERT-NLI\", \"RoBERTa\", \"XLM-RoBERTa\", \"BERT-Large\"]\n",
        "\n",
        "# Identify the Best Performing Model in Each Case\n",
        "best_model_equal = model_names[np.argmax(equal_weights)]\n",
        "best_model_custom = model_names[np.argmax(custom_weights)]\n",
        "\n",
        "# Print Results as Tables\n",
        "df_equal = pd.DataFrame({\n",
        "    \"Rank\": range(1, len(sentences) + 1),\n",
        "    \"S1\": [f\"S{i+1}\" for i in ranking_equal],\n",
        "    \"S2\": [f\"S{i+1}\" for i in ranking_equal],\n",
        "    \"TOPSIS Score\": [f\"{closeness_equal[i]:.4f}\" for i in ranking_equal]\n",
        "})\n",
        "\n",
        "df_custom = pd.DataFrame({\n",
        "    \"Rank\": range(1, len(sentences) + 1),\n",
        "    \"S1\": [f\"S{i+1}\" for i in ranking_custom],\n",
        "    \"S2\": [f\"S{i+1}\" for i in ranking_custom],\n",
        "    \"TOPSIS Score\": [f\"{closeness_custom[i]:.4f}\" for i in ranking_custom]\n",
        "})\n",
        "\n",
        "print(\"\\nRanking of Sentence Pairs (Equal Weights, Higher is Better):\")\n",
        "print(df_equal.to_string(index=False))\n",
        "\n",
        "print(\"\\nRanking of Sentence Pairs (Custom Weights, Higher is Better):\")\n",
        "print(df_custom.to_string(index=False))\n",
        "\n",
        "print(f\"\\nBest Performing Model (Equal Weights): {best_model_equal}\")\n",
        "print(f\"Best Performing Model (Custom Weights): {best_model_custom}\")\n"
      ]
    }
  ]
}